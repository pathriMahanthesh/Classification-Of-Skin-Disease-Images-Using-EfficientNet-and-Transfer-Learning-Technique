import os
from keras.models import Sequential
from keras.layers import Dense, GlobalAveragePooling2D

from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint
from keras.applications import EfficientNetB0
from keras.preprocessing.image import ImageDataGenerator

# Set directories
train_data_dir = r'E:\BITS PROJECT FINAL\OCT-DEC RBITS PROJECTS\2 [SVIT] SKIN DISEASE\training\Data\train'
test_data_dir = r'E:\BITS PROJECT FINAL\OCT-DEC RBITS PROJECTS\2 [SVIT] SKIN DISEASE\training\Data\Test'

# Set ImageDataGenerator for training data with validation split
train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2)  # Add validation split here

# Set ImageDataGenerator for test data (only rescaling)
test_datagen = ImageDataGenerator(rescale=1. / 255)

# Set batch size and image size
batch_size = 32
image_size = (224, 224)

# Load EfficientNetB0 without top layers
base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Create new model on top
model = Sequential()
model.add(base_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(256, activation='relu'))
model.add(Dense(9, activation='softmax'))  # Assuming you have 9 classes

# Compile the model
model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()


# Set up train and validation generators
train_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=image_size,
    batch_size=batch_size,
class_mode='categorical',
    subset='training')  # Specify subset for training data

validation_generator = train_datagen.flow_from_directory(
    train_data_dir,
    target_size=image_size,
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation')  # Specify subset for validation data

# Define checkpoint to save best weights
checkpoint = ModelCheckpoint('best_weights.keras', monitor='val_loss', save_best_only=True, mode='min', verbose=1)

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // batch_size,
    epochs=30,  # Adjust number of epochs as needed
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // batch_size,
    callbacks=[checkpoint])

# Save the trained model
model.save('2_effnet.keras')

